\documentclass[a4paper, 11pt]{article} 
\usepackage{parskip}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsfonts}

\title{Mathematical basis of the Quadratic Sieve}
\author{Alexandre Joly}

\begin{document}
\maketitle

\begin{abstract}
    This document present the Quadratic Sieve algorithm. First, we explain why we need an involving algorithm to factorize a large number. Then, we walk throught the mathematical basis of the algorithm.
\end{abstract}

\tableofcontents
\newpage


\section{RSA cryptosystem}

RSA is an asymmetric encryption system \textit{(public key for encryption and private key for decryption)}. It was published in 1977 by Ron Rivest, Adi Shamir and Leonard Adleman, from whose surnames the initials R, S and A are derived.

\subsection{Principle}
The aim of RSA is to build a \textbf{one-way} function, meaning it is easy to compute in one direction but computationally hard to reverse.

First, let's posit two required lemmas:
\begin{enumerate}
    \item \textbf{Bézout's Identity:} for any two integers $p$ and $q$, there exist integers $x$ and $y$ such that:
    \begin{equation}
        px+qy= \gcd(p,q) 
    \end{equation}
    Then if $p$ and $q$ are distinct prime numbers:
    \begin{equation}
        px+qy= 1
    \end{equation}
    \item \textbf{Euler's theorem (generalization of Fermat's Little Theorem)}: if $n$ is a positive integer and $a$ is an integer coprime with $n$, and $\varphi(n)$ is Euler's totient function, then:
    \begin{equation}
        a^{\varphi(n)} \equiv 1 \mod n
    \end{equation}
    The Euler's totient function can be computed using the Euler's product formula:
    \begin{equation}
        \varphi(n) = n \prod_{p|n}\left(1-\frac{1}{p}\right)
    \end{equation}
    where $p$ are the distinct prime factors of $n$.
\end{enumerate}

Now, given:
\begin{itemize}
    \item $n = p \times q$ with $p$ and $q$ distinct prime numbers
    \item $a$ an integer coprime with $n$
    % \item The Euler's totient $\varphi(n) =\varphi(pq) = pq \times \left(1-\frac{1}{p}\right) \times \left(1-\frac{1}{q}\right) = pq - q - p =(p-1)(q-1)$
    \item $e$ an integer coprime with $\varphi(n)$ 
\end{itemize}

Then, according to Bézout's identity, there exist two integers $d$ and $m$ such that:
\begin{equation}
    ed - m\varphi(n) = 1 \iff ed = 1 + m\varphi(n)
\end{equation}

Hence:
\begin{equation}
    a^{ed} = a^{1 + m\varphi(n)} = a \times (a^{\varphi(n)})^m
\end{equation}

Using Euler's theorem:
\begin{equation}
    a^{ed} \equiv a \times \left( (1 \mod n)^m  \right)
\end{equation}

Resulting in the \textbf{RSA theorem}:
\begin{equation}
    a^{ed} \equiv a
\end{equation}

Hence using $e$ and $d$ it is possible to encrypt and decrypt a message.


\subsection{Keys generation}

\begin{enumerate}
\item Choose two distinct large prime numbers: $p$ and $q$
\item Compute the product named the \textbf{modulus}: $n = p \times q$
\item Compute the \textit{Euler's totient function} in $n$: $\varphi(n)=\varphi(pq)=(p-1)(q-1)$ \footnote{While the original version of RSA use Euler's totient, modern implementation use Carmichael's function for efficiency but the process is the same as $\lambda(n)$ divide $\varphi(n)$.}
\item Choose an integer $e$ smaller and coprime to $\varphi(n)$, $e$ is called \textbf{encryption exponent} or \textbf{public key exponent}.
\item Compute $d$ the \textbf{decryption exponent} or \textbf{private key exponent} which is the \textit{modular multiplicative inverse} of $e$ modulo $\varphi(n)$: $d \equiv e^{-1} \mod \varphi(n)$. This is solved in the form $de \equiv 1 \mod \varphi(n)$ using the \textit{extended Euclidean algorithm}.
\end{enumerate}


Then we have both keys:
\begin{itemize}
    \item Public key: $\{n,e\}$
    \item Private key: $d$
\end{itemize}

The size of the modulus $n$ in bits is the key length. Nowadays, RSA system use 2048 or 4096 bits keys. To visualize how long it is we can use the logarithm to estimate the number of decimal digits in function of the key length $k$:
\begin{equation}
    2^k = 10^n \implies n = k \log_{10}(2)
\end{equation}

\begin{table}[h!]
\centering
    \begin{tabular}{|c|c|}
        \hline
        \textbf{Key Size (bits)} & \textbf{Approximate Number of Decimal Digits} \\
        \hline
        1024 & 308 \\
        2048 & 617 \\
        4096 & 1233 \\
        \hline
    \end{tabular}
\caption{RSA key size and corresponding number of decimal digits}
\end{table}

The record for the largest RSA number factorization is 829 bits, which is equivalent to 250 decimal digits. This record was achieved in 2020 by a team of researchers from the University of Limoges, using the General Number Field Sieve (GNFS) algorithm and 2700 CPU core-years (2.1 GHz Intel Xeon Gold 6130 as reference). 

\subsection{Encryption and decryption of a message}
Let the message $M$ be a natural number smaller than $n$.

The ciphertext is then: $C \equiv M^e \mod n$

To recover the initial message we need the private key: $C^d \equiv M^{ed} \equiv M \mod n$

\subsection{Reverse operation}
We saw that having the decryption exponent $d$ and the modulus $n$ can be used to decode the ciphertext. 

$d$ is found by solving $de \equiv 1 \mod \varphi(n)$. However, if $n$ is large and we don't know its factorisation then the computation of $\varphi(n)$ is too long.

Thus finding the factorisation $n = p \times q$ is the key to the key.

\section{Quadratic Sieve by Carl Pomerance (1981)}

The Quadratic Sieve is an \textbf{integer factorization algorithm} for large $N$. It was invented by Carl Pomerance in 1981 as an improvement to Schroeppel's linear sieve that builds on the ideas of Dixon and Kraitchik.

In practice, it is the second fastest algorithm for factorizing integers of more than 100 digits, after the General Number Field Sieve. It is, however, the fastest for integers of less than 100 digits. Given our resources, we will focus on the factorization of integers of less than 100 digits and therefore, the Quadratic Sieve algorithm is relevant.


\subsection{Principle}
In the field of factorization, \textit{Pierre de Fermat (17th century)} is a
major figure. He was the first to propose a method to factorize a number $N$
using the representation of an odd number as the difference of two squares:
\begin{equation}
    N = a^2 - b^2 = (a + b) \times (a - b)
\end{equation}

Fermat proposed to search the smallest $a \geq \lceil \sqrt{N} \rceil$ such that $a^2 - N$ is a perfect square, requiring $O(\sqrt{N})$ trials in the worst case. In its simplest form, Fermat's method might be slower than trial division but this idea inspired many other mathematicians, including \textit{Maurice Kraitchik}.

In the 1920s, Kraitchik proposed that instead of looking for a difference of
two squares $N = a^2 - b^2$, we could look for a difference of two squares that
is a multiple of $N$. In other words, a number that is a square modulo $N$:
\begin{equation}
    a^2 \equiv b^2 \mod{N}, \quad \text{with} \quad a \not\equiv \pm b \mod{N} \iff a^2 - b^2 \equiv 0 \mod{N}
\end{equation}
Equivalent to:
\begin{equation}
    \left\{
    \begin{array}{ll}
        (a - b) \times (a + b) \ \text{is a multiple of N} \\
        (a-b) \ \text{and} \ (a+b) \ \text{are not multiples of N (trivial case)}
    \end{array}
    \right.
\end{equation}
Thus, $N$ has a non-trivial factor with at least one of $(a-b)$ and $(a+b).$

\textbf{So, $\gcd(a+b,N)$ and/or $\gcd(a-b,N)$ are non-trivial factors of $N$.}

\subsection{Kraitchik Method}

To find squares $a^2$ and $b^2$ satisfying the necessary congruence conditions, Kraitchik computed values from what we now call the \textbf{Kraitchik polynomial}:
\begin{equation}
Q(x_i) = x_i^2 - N
\end{equation}

Kraitchik then sought combinations of these values that produce a perfect square. An effective strategy involves factoring each $Q(x_i)$ and identifying combinations such that the product of these factors yields even exponents.

Specifically, the combination yields the desired square:
\begin{equation}
    (x_1^2 - N)(x_2^2 - N) \dots (x_k^2 - N) = b^2
\end{equation}
This implies:
\begin{equation}
    \prod_{i=1}^{k} (x_i^2 - N) \equiv \prod_{i=1}^{k} x_i^2 \mod N, \quad \text{with} \quad a = \prod_{i=1}^{k} x_i
\end{equation}
Therefore, we have:
\begin{equation}
    a^2 \equiv b^2 \mod N
\end{equation}
which can then be exploited to factor the integer $N$.

\subsection{Dixon technique}

Before discussing Dixon's technique, we define the \textbf{smoothness of a number}. A number is said to be $B$-smooth if all its prime factors are less than or equal to $B$.

Dixon's key improvement was collecting only values $Q(x_i)$ that are $B$-smooth. With a suitably chosen small $B$, identifying combinations of these smooth values becomes significantly easier. We thus define a \textbf{factor base} consisting of all primes less than or equal to $B$ and denote its size by $\pi(B)$. The $B$-smooth values we select are termed \textbf{relations}.

Next, we construct a matrix $M$ where each row represents the exponent vector of prime factors for each relation $Q(x_i)$. Hence, the matrix $M$ has $\pi(B)$ columns, one for each prime in the factor base.

A key observation is that a perfect square has only even exponents in its prime factorization. Thus, if we find a combination of relations whose exponents sum to an even vector, we obtain a perfect square. Formally, this corresponds to finding a combination of rows of $M$ whose sum is a zero vector modulo 2.

To identify such combinations, we find the kernel of the transposed matrix $M^T$ over the finite field $\mathbb{F}_2$. A non-trivial kernel requires the rank of $M$ to be strictly less than the number of columns, necessitating at least $\pi(B)+1$ relations.

In general, finding the kernel of a matrix can be performed using Gaussian elimination, which has a complexity of $O(n^3)$. However, in this context, the matrix $M$ is typically large and sparse, motivating the use of more efficient methods specialized for sparse linear algebra.

\subsection{Schroeppel's linear sieve}
In 1977, Schroeppel proposed an important refinement of Dixon's random-sampling approach by replacing it with an efficient sieving process on a two-dimensional polynomial:
\begin{equation}
Q(i,j) = (r+i)(r+j) - N, \quad \text{where} \quad r = \bigl\lceil\sqrt{N}\bigr\rceil
\end{equation}

Sieving in two dimensions offers two main advantages. First, it generates a richer set of candidate values around zero—by varying both $i$ and $j$, one explores a full grid of residues, increasing the density of potential smooth numbers. Second, the 2D layout improves memory locality and allows parallel sieving across rows or columns, yielding practical speedups on modern hardware.

To clearly understand Schroeppel's sieve, we first recall the concept of \textbf{residue classes}. In modular arithmetic, the residue class modulo $n$ of an integer $a$ is the set of all integers congruent to $a$ modulo $n$:
\begin{equation}
[a]_n = { a + kn : k \in \mathbb{Z}}
\end{equation}
All elements within a residue class $[a]_n$ are indistinguishable modulo $n$.

Using this concept, the sieving proceeds as follows:

For each prime $p$ in our chosen factor base, we first solve the congruence
\begin{equation}
Q(x) \equiv 0 \mod p
\end{equation}
to find the two distinct roots $\alpha, \beta$ modulo $p$. Positions corresponding to these residue classes $x \equiv \alpha, \beta \mod p$ in a sieve array are marked (or "crossed off") by adding the value $\log p$.

The rationale behind adding logarithms is to efficiently approximate factorization. When a prime $p$ divides $Q(x)$, the logarithmic value of $|Q(x)|$ decreases by approximately $\log p$ for each occurrence of $p$ as a factor. We initialize an array $S[x]$ to zero, representing the logarithmic accumulations. Each time we identify a residue class modulo $p$, we increment by $\log p$:
\begin{equation}
S[x] \leftarrow S[x] + \log p
\end{equation}

After processing all primes in the factor base, we compare the accumulated values $S[x]$ with the logarithmic magnitude of $|Q(x)|$. If the total of recorded logarithms in $S[x]$ closely matches $\log|Q(x)|$ (within a small threshold accounting for rounding errors), this strongly indicates that $Q(x)$ factors completely over the factor base, identifying it as $B$-smooth. These identified smooth values are exactly the "relations" required to build the necessary matrix for subsequent steps.

This sieve reduces the cost of candidate testing dramatically and laid the groundwork for the first large-scale Quadratic Sieve implementations.

\subsection{Pomerance and the Quadratic Sieve}

Building on Schroeppel's two-dimensional linear sieve, Pomerance proposed a much more practical and efficient method by simplifying the polynomial used and refining the process of detecting $B$-smooth.

At the heart of Pomerance's method is a carefully chosen quadratic polynomial:
\begin{equation}
Q(x) = (x + r)^2 - N, \quad \text{where} \quad r = \bigl\lceil\sqrt{N}\bigr\rceil.
\end{equation}
This formulation ensures that $Q(x)$ is close to zero when $x$ is small, making it more likely that $Q(x)$ is $B$-smooth.

\subsubsection{Roots Modulo $p$ and Residue Classes}

A key insight lies in analyzing the congruence
\begin{equation}
(x + r)^2 \equiv N \mod p.
\end{equation}
This equation seeks values of $x$ for which $Q(x) \equiv 0 \mod p$. Solving it is equivalent to finding the square roots of $N$ modulo $p$. Specifically, we seek $s \in \mathbb{Z}$ such that:
\begin{equation}
s^2 \equiv N \mod p.
\end{equation}
If $N$ is a quadratic residue modulo $p$, then there exist exactly two such square roots, say $s$ and $-s \mod p$. Rewriting the original congruence:
\begin{equation}
x + r \equiv \pm s \mod p \quad \Longrightarrow \quad
x \equiv s - r \mod p \quad \text{or} \quad x \equiv -s - r \mod p,
\end{equation}
reveals two residue classes $\alpha_p, \beta_p$ for which $p \mid Q(x)$. These positions are used to initialize the sieve for each prime $p$ in the factor base.

\subsubsection{Sieve Centered Around $\sqrt{N}$}

The sieve is centered at $x = 0$, which corresponds to evaluating $Q(x)$ near $r = \lceil \sqrt{N} \rceil$. Around this point, we have the approximation:
\begin{equation}
Q(x) = (r + x)^2 - N = r^2 - N + 2rx + x^2 \approx 2rx + x^2.
\end{equation}
Since $r \approx \sqrt{N}$, the dominant term is linear in $x$ for small $x$. Thus, $Q(x)$ grows approximately linearly with $x$, meaning that the values of $Q(x)$ increase steadily and predictably as $x$ increases or decreases. This behavior keeps $|Q(x)|$ small in a neighborhood around $x = 0$, significantly increasing the probability that $Q(x)$ will be $B$-smooth.

\subsubsection{Choosing the Factor Base Using the Legendre Symbol}

To ensure efficiency, only those primes $p$ for which $N$ is a quadratic residue modulo $p$ are used in the factor base. This is efficiently tested via the Legendre symbol:
\begin{equation}
\Bigl(\tfrac{N}{p}\Bigr) = N^{\frac{p-1}{2}} \bmod p =
\begin{cases}
1, & \text{if } N \text{ is a quadratic residue mod } p, \\
-1, & \text{otherwise}.
\end{cases}
\end{equation}
This ensures that for each $p$ in the factor base, the congruence $(x + r)^2 \equiv N \mod p$ has exactly two solutions, and thus $Q(x)$ can potentially be divisible by $p$.

\subsubsection{Optimal Choice of the Smoothness Bound $B$}

Selecting the bound $B$ is a balancing act: it must be large enough to include enough primes for factoring $Q(x)$ values, but small enough to keep the linear algebra manageable. Pomerance suggested, after a study of the smooth number distribution, the following heuristic bound to minimize total complexity:
\begin{equation}
B = e^{\frac{1}{2} \sqrt{\log N \log \log N}}.
\end{equation}

\subsubsection{Key Improvements Beyond the Basic QS}

Pomerance and later researchers introduced several refinements that significantly improved the performance and practicality of the algorithm:
\begin{itemize}
\item \textbf{Self-initializing sieve:} Rather than hardcoding the positions $\alpha_p, \beta_p$ where each $p$ divides $Q(x)$, this approach dynamically computes them using the modular square roots of $N$ modulo $p$. These positions are then used to mark the sieve efficiently.
\item \textbf{Large-prime variation:} Instead of requiring $Q(x)$ to be fully $B$-smooth, relations with one or two larger primes in $(B, B^2)$ are accepted. These can be recombined later to form usable full relations, boosting the yield significantly.
\item \textbf{Multiple-polynomial variant (MPQS):} To extend the method to larger values of $N$, several different polynomials $(ax + b)^2 - N$ are used, each producing smaller $Q(x)$ values and allowing parallelization.
\item \textbf{Bucket/block sieving:} To reduce memory usage and improve cache efficiency, sieve updates are batched by prime in blocks or "buckets," which avoids touching all sieve positions for each $p$.
\end{itemize}

Initially, the quadratic sieve employed the Lanczos algorithm for linear algebra computations. However, contemporary implementations typically use Wiedemann's algorithm, which offers linear algebra solutions with complexity $O(n\omega)$, where $\omega$ is the number of nonzero entries.


\newpage

\section{Wiedemann's algorithm}
The \textbf{Wiedemann algorithm} \cite{wiedemann} is a probabilistic iterative, projection-based Krylov-subspace method for solving sparse systems of linear equations over finite fields. 

It is based on the fact that when a square matrix is repeatedly applied to a
vector, the sequence of vectors satisfies a linear recurrence.

Consider a sparse $n \times n$ matrix $A$ and a vector $b$ of size $n$ over a finite field $\mathbb{F}$, the problem is to find a vector $x$ such that:
\begin{equation}
    Ax = b
\end{equation}

\subsection{Origin}
The \textbf{Cayley-Hamilton theorem} asserts that the square matrix $A$ satisfies its own characteristic equation of degree $n$, giving the identity:
\begin{equation}
    P_A(A) = A^n + c_{n-1}A^{n-1} + \dots + c_1A + c_0I = 0
\end{equation}

However, the \textbf{minimal polynomial} may be of smaller degree $d \leq n$:
\begin{equation}
    \label{eqn:minimal}
    m_A(A) = A^d + m_{d-1}A^{d-1} + \dots + m_1A + m_0I = 0
\end{equation}

This minimal polynomial retains the same relation but with possibly fewer terms, we don't know $d$ yet but the smaller it is, the faster we will find a solution. Also note that this minimal polynomial is by definition monic (highest-degree coefficient equal to 1).

In the singular case the minimal polynomial has $m_0=0$, so $A$ is not invertible, thus the minimal polynomial is of the form:
\begin{equation}
    m_A(A) = A^d + m_{d-1}A^{d-1} + \dots + m_1A = 0
\end{equation}

We can therefore factor out $A$ at least once (more if $m_1, m_2, \dots, m_{d-1}$ are also zero):
\begin{equation}
    A(A^{d-1} + m_{d-1}A^{d-2} + \dots + m_1I) = 0
\end{equation}
 
We rewrite this as:
\begin{equation}
    m_A(A) = A^r q(A) = 0, \quad r \geq 1
\end{equation}

Hence, $q(A)$ is a member of the kernel of $A^r$:
\begin{equation}
    im(q(A)) \subseteq \ker(A^r) \iff im(q(A)) \subseteq \ker(A)
\end{equation}

Then we want to find a vector $w$ such that:
\begin{equation}
    w = q(A)v = \sum^d_{i=1} m_i A^i v
\end{equation}

\subsection{Build the vector $w$}

The definition above of $w$ is a sequence of vectors in the Krylov subspace of $A$ and $v$:
\begin{equation}
    \mathcal{K}_d(A,v) = \mathrm{span}\{v, Av, A^2v, \dots, A^{d-1}v\}
\end{equation}

Now, we need to find the coefficients $m_i$. Note that $m_i$ are field elements, so in our context, they are elements of $\mathbb{F}_2$.

The idea of Wiedemann is to do a \textbf{scalar projection} to turn that matrix problem over $\mathbb{F}^n$ into a one-dimensional recurrence in $\mathbb{F}$.

\begin{enumerate}
    \item Let's choose two random vectors $u$ and $v$ in $\mathbb{F}^{n}$.
    \item Define the scalar sequence of length $2n$ (we will see why next): $$s_i = u^TA^iv, \quad i \in [0, 2n-1]$$
    % As we built $\{s_i\}$ by multiplying the powers of $A$ to the right by $v$ and to the left by $u^T$, the minimal polynomial of $A$ will also annihilate the sequence $\{s_i\}$.
    % Therefore we reduced the $m_i$ search to a scalar sequence.
    \item Using the relation of the minimal polynomial we have:
    $$\sum^d_{i=0} m_i s_{i} = 0$$
    The only unknowns are the $d$ terms $m_i$ but at this time we only have 1 equation, which is not enough. In order to build $d$ equations we shift the identity by multiplying it by any non-negative power $A^k$ because it doesn't alter the identity:
    $$m_A(A)A^k=0, \quad k \ge 0$$
    Giving the scalar sequence:
    $$\sum^d_{i=0} m_i u^T A^{i+k}v = \sum^d_{i=0} m_i s_{k+i} = 0$$
    Now we have enough equation to find and fix $d$ terms $m_i$.
\end{enumerate}



\subsection{Berlekamp-Massey algorithm}

The \textbf{Berlekamp-Massey} algorithm, given a finite-length scalar sequence $\{s_0,s_1,\dots,s_{N-1}\}$, computes the shortest linear recurrence (i.e., the minimal polynomial) that generates it:
% In our context, once we have projected our Krylov-sequence to scalars $s_i=u^TA^iv$, Berlekamp-Massey will produce the monic polynomial:
\begin{equation}
    C(x) = 1 + C_1x + C_2x^2 + \dots + C_Lx^L
\end{equation}

However, for the moment we only know the value of $n$ but not $d$. Berlekamp-Massey can only be certain that no shorter recurrence exists once it has processed at least twice the true degree. That is why we will have to use a Krylov sequence of length $2n$.

\textbf{Algorithm:}

\begin{enumerate}
    \item Initialization:
    $$C(x)=1, \quad B(x) = 1, \quad T(x) = 1, \quad L = 0, \quad m = 1, \quad b = 1$$
    \begin{itemize}
        \item $C(x)$ is the current connection (candidate minimal) polynomial
        \item $B(x)$ is the previous version of $C$ at the last update
        \item $L$ is the current length (degree) of $C$
        \item $m$ counts how many steps since the last update
        \item $b$ stores the last nonzero discrepancy
    \end{itemize}
    \item Iteration, for each $s_i$: \\
    Compute the discrepancy $d$, it measures how far the current polynomial fails to annihilate the sequence at position $i$:
        $$d = s_i - \sum^L_{j=1}C_j s_{i-j}$$
    \begin{itemize}
        \item If $d=0$, the current minimal polynomial $C(x)$ correctly predicts the term $i$, so no update is needed, simply increment $m \leftarrow m + 1$.
        \item If $d \neq 0$, we correct $C$ by adding a shifted copy of the last “good” polynomial $B(x)$, scaled so as to cancel that error:
        \begin{itemize}
            \item Store a copy of the current minimal polynomial $C(x)$ as the previous minimal polynomial $T(x)$: $T(x) \leftarrow C(x)$
            \item Update the current minimal polynomial $C(x)$ as: $$C(x)=C(x)-\frac{d}{b}x^mB(x)$$
            \item if $2L \leq i$:
            \begin{itemize}
                \item Update the length $L$: $L\leftarrow i+1-L$
                \item Update the previous minimal polynomial $B(x) \leftarrow T(x)$, the scaling factor $b \leftarrow d$ and the step counter $m \leftarrow i + 1 - L$
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{enumerate}


\subsubsection{Complexity of Berlekamp-Massey:}

Each step requires $O(L)$ operations to compute $d$, resulting in a total $O(L^2)$ complexity.
$C(x)$ updates cost $O(L)$.

Therefore, the Berlekamp-Massey algorithm has a complexity of $O(d^2)$, with $d \leq n$.

\subsection{Probability of success}

Wiedemann's algorithm relies on the fact that the random vector $u$ chosen from $\mathbb{F}^n$ will project the Krylov sequence onto a space that captures the minimal polynomial of $A$.

It succeeds when the projected sequence $s_i=u^TA^iv$ has the same minimal polynomial as $A$. Failure occurs only if $u$ lies in the orthogonal complement $\mathcal{K}_d(A,v)^\perp$ because then the projection $u^TA^iv$ will not capture the full span of $\mathcal{K}_d(A,v)$.

Because $\dim\mathcal{K}_d^\perp=n-d$, the probability that a random $u$ lies in this orthogonal complement is given by the ratio of the dimensions:

\begin{equation}
p_{failure}=\frac{\dim\mathcal{K}_d^\perp}{\dim\mathbb{F}_2^n}=\frac{|\mathbb{F}_2|^{n-d}}{|\mathbb{F}_2|^n}=|\mathbb{F}_2|^{-d}
\end{equation}


Thus a single random $u$ already works with probability $1-|\mathbb{F}_2|^{-d}$, and repeating with fresh $u'$s drives the failure rate down exponentially: after $t$ trials it is $|\mathbb{F}_2|^{-dt}$.

In the Quadratic Sieve, $\mathbb{F} = \mathbb{F}_2$ and $d$ is typically small, so the probability of success is very high with just a few random vectors.

Also, in the case of singular matrices, the minimal polynomial has $m_0=0$, so in order to ensure Berlekamp-Massey finds the minimal polynomial such as $m_A(0) = 0$, we need to ensure that $s_0 = u^Tv = 0$. This can be easily checked and if not satisfied, we can simply choose a new random $u$.

\subsection{Complexity of Wiedemann's Algorithm:}

We saw that the complexity of the Berlekamp-Massey algorithm is $O(d^2)$, the other major computation is the construction of the Krylov subspace.

It requires $n$ matrix-vector multiplication that costs $O(\omega)$ field ops where $\omega$ is the number of non-zeros in $A$. Giving a complexity of $O(n\omega)$.

For very sparse matrices, as in the Quadratic Sieve, it is typical that $d \ll \omega$, so the Krylov subspace construction dominate and give an overall complexity of $O(n\omega)$.

\subsection{Conclusion}
Wiedemann's algorithm is a powerful method for solving large sparse linear systems over finite fields, particularly in the context of exact linear algebra tasks. By leveraging the properties of the minimal polynomial and the Berlekamp-Massey algorithm, it efficiently reduces the problem to a manageable scalar sequence, allowing for the recovery of solutions with a relatively low computational cost.

\section{Example}

Consider the following $7 \times 7$ square sparse matrix $M$ over $\mathbb{F}_2$ that could arise in the Quadratic Sieve context:
\begin{equation}
    M = \begin{pmatrix}
        0 & 0 & 0 & 0 & 0 & 0 & 0 \\
        0 & 1 & 0 & 0 & 0 & 1 & 1 \\
        0 & 0 & 1 & 1 & 1 & 1 & 0 \\
        0 & 0 & 1 & 1 & 0 & 0 & 0 \\
        0 & 0 & 1 & 0 & 1 & 1 & 0 \\
        0 & 1 & 1 & 0 & 1 & 0 & 1 \\
        0 & 1 & 0 & 0 & 0 & 1 & 1 \\
    \end{pmatrix}
\end{equation}

\begin{enumerate}
    \item First, let's check that the $M$ has a rank smaller than 7. We can see that columns 2 and 7 are equal, so they are linearly dependent. We can also see that the first column is all zeros, which means it does not contribute to the rank. Therefore, the rank of $M$ is at most 5. Hence, this matrix has a non-trivial kernel. We can apply the Wiedemann's algorithm to find a solution to the system $Mx = 0$.
    \item We choose the random vector $v$ and $u$, for example:
    \begin{equation}
        v = \begin{pmatrix}
            0 \\
            0 \\
            1 \\
            1 \\
            1 \\
            0 \\
            1 \\
        \end{pmatrix}
        \quad \text{and} \quad
        u = \begin{pmatrix}
            0 \\
            0 \\
            0 \\
            1 \\
            1 \\
            0 \\
            0 \\
        \end{pmatrix}
    \end{equation}
    We check that $s_0=u^Tv \equiv 0 \mod 2$:
    \begin{equation}
        s_0 = u^Tv = 0 \cdot 0 + 0 \cdot 0 + 0 \cdot 1 + 1 \cdot 1 + 1 \cdot 1 + 0 \cdot 0 + 0 \cdot 1 = 2 \equiv 0 \mod 2
    \end{equation}

    \item We compute the scalar sequence $s_i = u^T M^i v$ for $i=0,1,\dots,13$:
    \begin{equation}
        \{s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9, s_{10}, s_{11}, s_{12}, s_{13} \} 
        = 
        \left\{0,0,1,0,1,0,0,1,0,1,0,0,1,0\right\}
    \end{equation}
    \item We apply the Berlekamp-Massey algorithm to the scalar sequence $\{s_i\}$ to find the minimal polynomial coefficients $m_i$:
    \begin{equation}
        m_i = \{1, 1, 1, 1, 1, 0\}
    \end{equation}
    \item The minimal polynomial is then:
    \begin{equation}
        m_M(x) = 1 + x + x^2 + x^3 + x^4
    \end{equation}
    \item We can now compute the solution $x$ using the minimal polynomial:
    \begin{equation}
        x = -(M^4 v + M^3 v + M^2 v + M v) \mod 2
        = \begin{pmatrix}
            0 \\
            1 \\
            0 \\
            0 \\
            1 \\
            1 \\
            0 \\
        \end{pmatrix}
    \end{equation}
    \item Finally, we can verify that $Mx = 0$:
    \begin{equation}
        Mx = 0
    \end{equation}
    This confirms that the Wiedemann's algorithm has successfully found a solution to the system $Mx = 0$.
\end{enumerate}

\newpage

\section{Parallelize sieving}
Here is the pseudocode of serial style seiving:

\begin{verbatim}
    Function Q_B_friables_128(N, A, factor_base):
    # Compute starting offset: sqrt(N) + 1
    range_start ← integer_sqrt(N) + 1

    # Initialize arrays
    Q  ← new array of length A         # holds Q(x) = (x + range_start)^2 - N
    Qf ← empty list                    # will hold the "fully factored" values
    X  ← empty list                    # will hold the corresponding x-values

    # 1. Fill the Q array
    For i from 0 to A-1 do
        x ← range_start + i
        Q[i] ← x * x - N

    # 2. For each prime in the factor base, strip out its powers
    For each p in factor_base do
        a1 ← -1
        a2 ← -1

        # Find the two first indices where Q[j] = 0 mod p
        For j from 0 to A-1 do
            If Q[j] mod p = 0 then
                If a1 = -1 then
                    a1 ← j
                Else
                    a2 ← j
                    Break out of the j-loop
        # Compute the gap between the two roots
        gap ← a2 - a1

        # Walk through the interval in steps of p, dividing out all p-factors
        For k from a1 to (A-1 - gap) step p do
            # Remove p-factors at position k
            While Q[k] > 1 and Q[k] mod p = 0 do
                Q[k] ← Q[k] / p
            # Remove p-factors at position k + gap
            While Q[k + gap] > 1 and Q[k + gap] mod p = 0 do
                Q[k + gap] ← Q[k + gap] / p

    # 3. Collect the "friable" values (where Q[i] has been reduced to 1)
    For i from 0 to A-1 do
        If Q[i] = 1 then
            x ← range_start + i
            original_Q ← x * x - N
            Append original_Q to Qf
            Append x to X

    # Return the list of friable values and their corresponding x's
    Return (Qf, X)

\end{verbatim}

We can see some parallelizable tasks:
\begin{itemize}
    \item Q filling: completely independent writes, the filling can be parallel
    \item Q[k] dividing
    \item Qf and X filling if we init them first
\end{itemize}


\newpage

\appendix

\section{Wiedemann's algorithm pseudo-code}
In pseudo-code, Wiedemann's algorithm can be summarized as follows:
\begin{verbatim}
function Wiedemann(A, b):
    n = len(b)
    u = random_row_vector(n)
    s = [0] * (2 * n)  # Initialize scalar sequence of length 2n
    for i in range(2 * n):
        s[i] = u @ (A ** i) @ b  # Compute scalar sequence
    C = BerlekampMassey(s)  # C contains the coefficients of the minimal polynomial
    x = compute_solution(A, b, C)  # Use C to compute the solution
    return x  # Return the solution vector x
\end{verbatim}

\section{Berlekamp-Massey algorithm pseudo-code}
In pseudo-code, the Berlekamp-Massey algorithm can be summarized as follows:
\begin{verbatim}
function BerlekampMassey(s):
    C = [1]  # current polynomial coefficients
    B = [1]  # previous polynomial coefficients
    L = 0    # current length of C
    m = 1    # step counter since last update
    b = 1    # last nonzero discrepancy
    for i from 0 to len(s) - 1:
        d = s[i] + sum(C[j] * s[i - j] for j in range(1, L + 1))
        if d == 0:
            m += 1
        else:
            T = C[:]  # store current C as T
            C = [C[j] - (d / b) * (C[j - m] if j >= m else 0) for j in range(len(C))]
            if 2 * L <= i:
                L = i + 1 - L
                B = T[:]
                b = d
                m = i + 1 - L
    return C  # returns the coefficients of the minimal polynomial
\end{verbatim}

\bibliographystyle{plain}
\bibliography{biblio}

\end{document}
